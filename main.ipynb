{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initialize\n",
    "from logistic_regression import logistic_regression\n",
    "from artificial_neural_network import ann_no_validation\n",
    "from kernel_svm import kernel_svm\n",
    "from lstm import lstm_no_validation, parse_lstm_data\n",
    "from naive_bayes import naive_bayes\n",
    "from random_forest import random_forest\n",
    "from artificial_neural_network_regression import ann_regression\n",
    "from lstm_regression import lstm_regression\n",
    "from multiple_linear_regression import multiple_linear_regression\n",
    "from polynomial_regression import polynomial_regression\n",
    "from random_forest_regression import random_forest_regression\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chart(title, x_label, y_label):\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.title(title)\n",
    "    plt.savefig('charts/{}.png'.format(title.replace(' ','_').lower()), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(title, x_label, y_label, data):\n",
    "    plt.hist(data, density=False, bins=20)  # density=False would make counts\n",
    "    plot_chart(title, x_label, y_label)\n",
    "    \n",
    "def plot_bar(title, x_label, y_label, x_data, y_data):\n",
    "    ax= plt.subplot()\n",
    "    plt.bar(x_data, y_data) \n",
    "    # plt.setp(ax.get_xticklabels(), rotation=30, ha='right')\n",
    "    plt.xticks(fontsize=10, rotation=90)\n",
    "    plot_chart(title, x_label, y_label)\n",
    "    \n",
    "def plot_pie_chart(title, labels, data):\n",
    "    data_converted = np.unique(data, return_counts=True)[1]\n",
    "    plt.pie(data_converted, labels = labels, startangle = 90, shadow = True, autopct='%.2f%%')\n",
    "    plt.title(title)\n",
    "    plt.savefig('charts/{}.png'.format(title.replace(' ','_').lower()), dpi=300)\n",
    "    plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_game_with_odds(game, bet_value):\n",
    "    game_money = 0\n",
    "    if (game['ODDS_A'] <= game['ODDS_B'] and game['ODDS_A'] > threshold) or (game['ODDS_A'] > game['ODDS_B'] and game['ODDS_B'] > threshold):\n",
    "        prediction = game['ODDS_A'] <= game['ODDS_B']\n",
    "        if game['WINNER'] == prediction and game['WINNER'] == 1:\n",
    "            game_money = (bet_value*game['ODDS_A'] - bet_value)\n",
    "        elif game['WINNER'] == prediction and game['WINNER'] == 0:\n",
    "            game_money = (bet_value*game['ODDS_B'] - bet_value)\n",
    "        else:\n",
    "            game_money = -bet_value\n",
    "    return game_money\n",
    "    \n",
    "def check_game_with_matchups(game, bet_value):\n",
    "    game_money = 0\n",
    "    if (game['MATCHUP_A'] > game['MATCHUP_B'] and game['ODDS_A'] > threshold) or (game['MATCHUP_A'] < game['MATCHUP_B'] and game['ODDS_B'] > threshold):\n",
    "        prediction = game['MATCHUP_A'] > game['MATCHUP_B']\n",
    "        if game['WINNER'] == prediction and game['WINNER'] == 1:\n",
    "            game_money = (bet_value*game['ODDS_A'] - bet_value)\n",
    "        elif game['WINNER'] == prediction and game['WINNER'] == 0:\n",
    "            game_money = (bet_value*game['ODDS_B'] - bet_value)\n",
    "        else:\n",
    "            game_money = -bet_value\n",
    "    return game_money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_performance_on_game_lstm(game, prediction, bet_value):\n",
    "    game_money = 0\n",
    "    if (prediction == 1 and game[1] > threshold) or (prediction == 0 and game[2] > threshold):\n",
    "        if game[3] == prediction and game[3] == 1:\n",
    "            game_money = (bet_value*game[1] - bet_value)\n",
    "        elif game[3] == prediction and game[3] == 0:\n",
    "            game_money = (bet_value*game[2] - bet_value)\n",
    "        else:\n",
    "            game_money = -bet_value\n",
    "    return game_money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_performance_on_game(game_lstm, prediction, bet_value):\n",
    "    game_money = 0\n",
    "    if (prediction == 1 and game['ODDS_A'] > threshold) or (prediction == 0 and game['ODDS_B'] > threshold):\n",
    "        if game['WINNER'] == prediction and game['WINNER'] == 1:\n",
    "            game_money = (bet_value*game['ODDS_A'] - bet_value)\n",
    "        elif game['WINNER'] == prediction and game['WINNER'] == 0:\n",
    "            game_money = (bet_value*game['ODDS_B'] - bet_value)\n",
    "        else:\n",
    "            game_money = -bet_value\n",
    "    return game_money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_neural_net(model_name, model):\n",
    "    try:\n",
    "        # load json and create model\n",
    "        json_file = open('models/{}.json'.format(model_name), 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        res = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        res.load_weights(\"models/{}.h5\".format(model_name))\n",
    "    except:\n",
    "        res = model(season)[0]\n",
    "        # serialize model to JSON\n",
    "        model_json = res.to_json()\n",
    "        with open(\"models/{}.json\".format(model_name), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        res.save_weights(\"models/{}.h5\".format(model_name))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bet_value(prob):\n",
    "    bet_value = 0 \n",
    "    if prob >= 0.5:\n",
    "        bet_value = 10*prob\n",
    "    else:\n",
    "        bet_value = 10*abs(1-prob)\n",
    "    return bet_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = '2008-2017'\n",
    "season_test = '2018-2018'\n",
    "results = []\n",
    "\n",
    "print('\\nGetting data for the 2018 season for testing...')\n",
    "my_path = os.path.abspath(os.path.dirname(__file__))\n",
    "\n",
    "dataset_train = pd.read_csv(os.path.join(my_path, 'data/seasons/winner/{}.csv'.format(season)))\n",
    "dataset_lstm = pd.read_csv(os.path.join(my_path, 'data/seasons/winner/LSTM/{}.csv'.format(season_test)))\n",
    "path = os.path.join(my_path, 'data/seasons/winner/{}.csv'.format(season_test))\n",
    "dataset = pd.read_csv(path)\n",
    "\n",
    "X = dataset.iloc[:, 5:-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "X_lstm = dataset_lstm.iloc[:, 1:-1].values\n",
    "y_lstm = dataset_lstm.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nExecuting the logistic Regression model...')\n",
    "Pkl_Filename = \"models/LogisticRegressionModel.pkl\"  \n",
    "try:\n",
    "    with open(Pkl_Filename, 'rb') as file:  \n",
    "        logisticRegression = pickle.load(file)\n",
    "except:\n",
    "    logisticRegression = logistic_regression(season, True)\n",
    "    with open(Pkl_Filename, 'wb') as file:  \n",
    "        pickle.dump(logisticRegression, file)\n",
    "results.append(dict(model='logistic Regression',cm=logisticRegression[0], acc=logisticRegression[1], classifier=logisticRegression[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Executing the Kernel SVM model...')\n",
    "Pkl_Filename = \"models/KernelSVM.pkl\"  \n",
    "try:\n",
    "    with open(Pkl_Filename, 'rb') as file:  \n",
    "        res = pickle.load(file)\n",
    "except:\n",
    "    res = kernel_svm(season, True)\n",
    "    with open(Pkl_Filename, 'wb') as file:  \n",
    "        pickle.dump(res, file)\n",
    "results.append(dict(model='Kernel SVM',cm=res[0], acc=res[1], classifier=res[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Executing the Naive Bayes model...')\n",
    "Pkl_Filename = \"models/NaiveBayes.pkl\"  \n",
    "try:\n",
    "    with open(Pkl_Filename, 'rb') as file:  \n",
    "        res = pickle.load(file)\n",
    "except:\n",
    "    res = naive_bayes(season, True)\n",
    "    with open(Pkl_Filename, 'wb') as file:  \n",
    "        pickle.dump(res, file)\n",
    "results.append(dict(model='Naive Bayes',cm=res[0], acc=res[1], classifier=res[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Executing the Random Forest model...')\n",
    "Pkl_Filename = \"models/RandomForest.pkl\"  \n",
    "try:\n",
    "    with open(Pkl_Filename, 'rb') as file:  \n",
    "        res = pickle.load(file)\n",
    "except:\n",
    "    res = random_forest(season, True)\n",
    "    with open(Pkl_Filename, 'wb') as file:  \n",
    "        pickle.dump(res, file)\n",
    "results.append(dict(model='Random Forest',cm=res[0], acc=res[1], classifier=res[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Executing the Artificial Neural Network model...')\n",
    "res = load_neural_net(\"ANN\", ann_no_validation)\n",
    "results.append(dict(model='ANN', classifier=res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Executing the LSTM model...')\n",
    "res = load_neural_net(\"LSTM\", lstm_no_validation)\n",
    "results.append(dict(model='LSTM', classifier=res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchups_baseline = dataset_train[((dataset_train['MATCHUP_A'] >= dataset_train['MATCHUP_B']) & (dataset_train['WINNER'] == 1)) | \n",
    "                   ((dataset_train['MATCHUP_B'] > dataset_train['MATCHUP_A']) & (dataset_train['WINNER'] == 0))]\n",
    "odds_baseline = dataset_train[((dataset_train['ODDS_A'] <= dataset_train['ODDS_B']) & (dataset_train['WINNER'] == 1)) | \n",
    "                   ((dataset_train['ODDS_B'] < dataset_train['ODDS_A']) & (dataset_train['WINNER'] == 0))]\n",
    "\n",
    "print('\\nResults Classification ({}):'.format(season))\n",
    "results.sort(key=lambda x: x['acc'], reverse=True)\n",
    "[print('{}:\\t{:.4f}'.format(x['model'], x['acc'])) for x in results]\n",
    "print('Baseline Last Machups:\\t{:.4f}'.format(100*len(matchups_baseline.index)/len(dataset_train.index)))\n",
    "print('Baseline Odds:\\t{:.4f}'.format(100*len(odds_baseline.index)/len(dataset_train.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nGetting the feature correlation matrix...')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    dependent_variables = dataset.iloc[:,5:20]\n",
    "    corrmat = dependent_variables.corr()\n",
    "    top_corr_features = corrmat.index\n",
    "    plt.figure(figsize=(13,13))\n",
    "    title = 'Feature Correlation'\n",
    "    plt.title(title)\n",
    "    #plot heat map\n",
    "    sns.set(font_scale=0.6)\n",
    "    g=sns.heatmap(dependent_variables.corr(),annot=True,cmap='Blues', fmt='0.1g')\n",
    "    plt.savefig('charts/{}.png'.format(title.replace(' ','_').lower()), dpi=300)\n",
    "    plt.show()\n",
    "except:\n",
    "    print('No correlation matrix for the selected model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nGetting classification model with the best predictions...')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_transformed = sc.fit_transform(X)\n",
    "\n",
    "modelCont = 0\n",
    "highestAcc = 0\n",
    "probs = dict()\n",
    "total_invested = dict()\n",
    "while True:\n",
    "    try:\n",
    "        total_invested[results[modelCont]['model']] = 0\n",
    "        if results[modelCont]['model'] == 'LSTM':\n",
    "            features_lstm, labels_lstm, info_lstm = parse_lstm_data(X_lstm, y_lstm)\n",
    "            pred = results[modelCont]['classifier'].predict(features_lstm)\n",
    "            results[modelCont]['pred'] = pred\n",
    "            probs['LSTM'] =  pred\n",
    "            pred_winner = (results[modelCont]['pred'] > 0.5)\n",
    "            results[modelCont]['pred_winner'] = pred_winner\n",
    "            results[modelCont]['acc_test'] = accuracy_score(labels_lstm, results[modelCont]['pred_winner'])\n",
    "        else:\n",
    "            results[modelCont]['pred'] = results[modelCont]['classifier'].predict(X_transformed)\n",
    "            if results[modelCont]['model'] == 'ANN':\n",
    "                probs['ANN'] =  results[modelCont]['pred']\n",
    "                results[modelCont]['pred'] = (results[modelCont]['pred'] > 0.5)\n",
    "            else:\n",
    "                probs[results[modelCont]['model']] =  results[modelCont]['classifier'].predict_proba(X_transformed)\n",
    "            results[modelCont]['acc_test'] = accuracy_score(y, results[modelCont]['pred'])\n",
    "        if results[modelCont]['acc_test'] > highestAcc:\n",
    "            y_pred = results[modelCont]['pred']\n",
    "            highestAcc = results[modelCont]['acc_test']\n",
    "            print('Using predictions from {} model: {}'.format(results[modelCont]['model'], results[modelCont]['acc_test']))\n",
    "        modelCont += 1\n",
    "    except IndexError:\n",
    "        break\n",
    "\n",
    "results.sort(key=lambda x: x['acc_test'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchups_baseline = dataset[((dataset['MATCHUP_A'] >= dataset['MATCHUP_B']) & (dataset['WINNER'] == 1)) | \n",
    "                   ((dataset['MATCHUP_B'] > dataset['MATCHUP_A']) & (dataset['WINNER'] == 0))]\n",
    "odds_baseline = dataset[((dataset['ODDS_A'] <= dataset['ODDS_B']) & (dataset['WINNER'] == 1)) | \n",
    "                   ((dataset['ODDS_B'] < dataset['ODDS_A']) & (dataset['WINNER'] == 0))]\n",
    "\n",
    "print('\\nResults Classification ({}):'.format(season_test))\n",
    "results.sort(key=lambda x: x['acc_test'], reverse=True)\n",
    "[print('{}:\\t{:.4f}'.format(x['model'], x['acc_test'])) for x in results]\n",
    "print('Baseline Last Machups:\\t{:.4f}'.format(100*len(matchups_baseline.index)/len(dataset.index)))\n",
    "print('Baseline Odds:\\t{:.4f}'.format(100*len(odds_baseline.index)/len(dataset.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nGetting the probabilities of the best model possible...')\n",
    "\n",
    "for res in results:\n",
    "    try:\n",
    "        y_prob = res['classifier'].predict_proba(X_transformed)\n",
    "        print('Using the {} model for probability tracking!'.format(res['model']))\n",
    "        break\n",
    "    except AttributeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDisplaying data for the {} model...'.format(results[0]['model']))\n",
    "cm = confusion_matrix(y.ravel(), y_pred.ravel())\n",
    "acc_score = accuracy_score(y, y_pred)\n",
    "print(cm)\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGetting data from the regular models for visualization...\")\n",
    "profit = 0\n",
    "money_by_date = []\n",
    "bets_tracking_matchups = [0]\n",
    "bets_tracking_odds = [0]\n",
    "money_by_team = dict()\n",
    "bets = []\n",
    "index_lstm = 0\n",
    "pred_winner_lstm = [x['pred_winner'] for x in results if x['model'] == 'LSTM'][0]\n",
    "money_by_date.append([dataset.iloc[0,2], dict(zip([x['model'] for x in results], [0 for x in results])),  dict(zip([x['model'] for x in results], [0 for x in results]))])\n",
    "for index, game in dataset.iterrows():\n",
    "    if game['GAME_DATE'] != money_by_date[-1][0]:    \n",
    "        bets_tracking_matchups.append(bets_tracking_matchups[-1])\n",
    "        bets_tracking_odds.append(bets_tracking_odds[-1])\n",
    "        money_by_date.append([game['GAME_DATE'],  dict(zip([x['model'] for x in results], [0 for x in results])), dict(money_by_date[-1][2])])\n",
    "\n",
    "    game_money = 0\n",
    "    bet_value = get_bet_value(y_prob[index,0])\n",
    "    if (y_pred[index] == 1 and game['ODDS_A'] > threshold) or (y_pred[index] == 0 and game['ODDS_B'] > threshold):\n",
    "        if game['TEAM_A'] not in money_by_team:\n",
    "            money_by_team[game['TEAM_A']] = 0\n",
    "        if game['TEAM_B'] not in money_by_team:\n",
    "            money_by_team[game['TEAM_B']] = 0\n",
    "\n",
    "        game_money = check_model_performance_on_game(game, y_pred[index], bet_value)\n",
    "        if game['WINNER'] == y_pred[index] and game['WINNER'] == 1:\n",
    "            bets.append(['A', game['ODDS_A'], y_prob[index,1], 1])\n",
    "            money_by_team[game['TEAM_A']] += game_money\n",
    "        elif game['WINNER'] == y_pred[index] and game['WINNER'] == 0:\n",
    "            bets.append(['B', game['ODDS_B'], y_prob[index,0], 1])\n",
    "            money_by_team[game['TEAM_B']] += game_money\n",
    "        else:\n",
    "            if y_pred[index] == 1:\n",
    "                bets.append(['A', game['ODDS_A'], y_prob[index,1], 0])\n",
    "                money_by_team[game['TEAM_A']] += game_money\n",
    "            else:\n",
    "                bets.append(['B', game['ODDS_B'], y_prob[index,0], 0])\n",
    "                money_by_team[game['TEAM_B']] += game_money\n",
    "\n",
    "    profit += game_money\n",
    "    bets_tracking_matchups[-1] += check_game_with_matchups(game, bet_value)\n",
    "    bets_tracking_odds[-1] += check_game_with_odds(game, bet_value)\n",
    "\n",
    "    for model in money_by_date[-1][1]:\n",
    "        if model == 'LSTM':\n",
    "            if index_lstm < len(info_lstm) and info_lstm[index_lstm][0] == money_by_date[-1][0]:\n",
    "                bet_value = get_bet_value(probs[model][index_lstm,0])\n",
    "                prediction = pred_winner_lstm[index_lstm]\n",
    "                game_money_model = check_model_performance_on_game_lstm(info_lstm[index_lstm], prediction, bet_value)\n",
    "                index_lstm += 1\n",
    "            else:\n",
    "                game_money_model = 0\n",
    "        else:\n",
    "            bet_value = get_bet_value(probs[model][index,0])\n",
    "            prediction = next(x['pred'][index] for x in results if x['model'] == model)\n",
    "            game_money_model = check_model_performance_on_game(game, prediction, bet_value)\n",
    "        if game_money_model != 0:\n",
    "            total_invested[model] += bet_value\n",
    "        money_by_date[-1][1][model] += game_money_model\n",
    "        money_by_date[-1][2][model] += game_money_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nProfit and margin by model...')\n",
    "for model in money_by_date[-1][1]:\n",
    "    print('Model: {} \\t// Invested: {} \\t// Won: {} \\t// Margin: {:.2f}%'.format(model, total_invested[model], money_by_date[-1][2][model], 100*money_by_date[-1][2][model]/total_invested[model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPlotting charts...')\n",
    "\n",
    "models_tracking =  [np.array([x[2][model] for x in money_by_date], dtype=np.float32) for model in money_by_date[-1][1]]\n",
    "\n",
    "money_by_date = np.array(money_by_date, dtype=str)\n",
    "correct_bets = list(filter(lambda x: x[3] == 1, bets))\n",
    "missed_bets = list(filter(lambda x: x[3] == 0, bets))\n",
    "correct_bets_odds = np.array(list(map(lambda x: x[1], correct_bets)))\n",
    "missed_bets_odds = np.array(list(map(lambda x: x[1], missed_bets)))\n",
    "# correct_bets_prob = np.array(list(map(lambda x: x[2], correct_bets)))\n",
    "# missed_bets_prob = np.array(list(map(lambda x: x[2], missed_bets)))\n",
    "correct_bets_home = np.array(list(map(lambda x: x[0], correct_bets)))\n",
    "missed_bets_home = np.array(list(map(lambda x: x[0], missed_bets)))\n",
    "\n",
    "money_by_team = dict(sorted(money_by_team.items(), key=lambda x: x[1]))\n",
    "money_by_team_labels = np.array(list(money_by_team.keys()), dtype=str)\n",
    "money_by_team_values = np.array(list(money_by_team.values()), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist('Missed Bets by Odds', 'Odds', 'X Times', missed_bets_odds)\n",
    "\n",
    "plot_hist('Correct Bets by Odds', 'Odds', 'X Times', correct_bets_odds)\n",
    "\n",
    "plot_pie_chart('Correct Bets by Home-Away', ['Home', 'Away'], correct_bets_home)\n",
    "\n",
    "plot_pie_chart('Missed Bets by Home-Away', ['Home', 'Away'], missed_bets_home)\n",
    "\n",
    "plot_bar('Profit by Team', 'Teams', 'Profit', money_by_team_labels, money_by_team_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = money_by_date[:,0].astype(np.datetime64)\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=15))\n",
    "for model in models_tracking:\n",
    "    plt.plot(xpoints, model)\n",
    "plt.plot(xpoints, bets_tracking_matchups)\n",
    "plt.plot(xpoints, bets_tracking_odds)\n",
    "\n",
    "title = \"Profit by Date\"\n",
    "plt.legend([x['model'] for x in results] + ['Matchups Baseline', 'Odds Baseline'], loc='lower left')\n",
    "plt.ylabel(\"Profit($)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.title(title)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.savefig('charts/{}.png'.format(title.replace(' ','_').lower()), dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
