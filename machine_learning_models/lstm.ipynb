{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"LSTM.ipynb"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Automatically generated by Colaboratory."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Original file is located at\n", "    https://colab.research.google.com/drive/19NQOKEK4GUOpMOh9TDJ6NXjyNVafDuXI\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import tensorflow as tf\n", "import matplotlib.pyplot as plt\n", "import keras\n", "import os.path"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def parse_lstm_data(X, y, timesteps=10):\n", "    tracking = []\n", "    info_tracking = []\n", "    features = []\n", "    labels = []\n", "    \n", "    info = np.concatenate((X[:,[2]], X[:,-2:]), axis = 1)\n", "    info = np.c_[ info[:,:], y ]  \n", "    \n", "    \" Feature Scaling \"\n", "    from sklearn.preprocessing import StandardScaler\n", "    sc = StandardScaler()\n", "    X[:,3:] = sc.fit_transform(X[:,3:])\n", "    \n", "    # print('Parsing the data to LSTM format...')\n", "    for i in range(2, len(X), 2):\n", "        team_a_id = X[i-2,1]\n", "        team_b_id = X[i-1,1]\n", "        team_a_abbv = X[i-2,0]\n", "        team_b_abbv = X[i-1,0]\n", "        # print('{}: {} x {}. Team A won? {}'.format(i, team_a_abbv, team_b_abbv, y[i-2]))\n", "        team_a_previous_games = X[(X[:,1] == team_a_id) & (X[:,2] < X[i-1,2]),:]\n", "        team_b_previous_games = X[(X[:,1] == team_b_id) & (X[:,2] < X[i-1,2]),:]\n", "        if len(team_a_previous_games) >= timesteps and len(team_b_previous_games) >= timesteps:\n", "            game_tracking = np.concatenate((team_a_previous_games[-1*timesteps:, 1:], team_b_previous_games[-1*timesteps:, 1:]), axis = 1)\n", "            game = np.concatenate((team_a_previous_games[-1*timesteps:, 3:], team_b_previous_games[-1*timesteps:, 3:]), axis = 1)\n", "            tracking.append(game_tracking)\n", "            features.append(game)\n", "            info_tracking.append(info[i-2,:])\n", "            labels.append(y[i-2])\n", "            \n", "    features = np.array(features).astype(np.float32)\n", "    labels = np.array(labels).astype(np.float32)\n", "    return features, labels, info_tracking"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def build_lstm(X_train, y_train, X_test = None, y_test = None):\n", "    \" Building the LSTM \"\n", "    \n", "    lstm = keras.Sequential()\n", "    lstm.add(keras.layers.LSTM(12, input_shape=(X_train.shape[1], X_train.shape[2])))\n", "    lstm.add(keras.layers.Dropout(0.9))\n", "    \n", "    # Output layer\n", "    lstm.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n", "    \n", "    \" Compiling the LSTM \"\n", "    optimiser = tf.keras.optimizers.Adam()\n", "    lstm.compile(optimizer=optimiser,\n", "                  loss='binary_crossentropy',\n", "                  metrics=['accuracy'])\n", "    lstm.summary()\n", "    \n", "    \" Training the LSTM on the Training set \"\n", "    \n", "    if X_test != None:\n", "        history = lstm.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = 32, epochs = 100)\n", "    else:\n", "        history = lstm.fit(X_train, y_train, batch_size = 32, epochs = 100)\n", "        \n", "    return lstm, history"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def import_dataset(season = '2018-2018'):\n", "    \n", "    \" Importing the dataset \"\n", "    \n", "    my_path = os.path.abspath(os.path.dirname(__file__))\n", "    path = os.path.join(my_path, '../../data/seasons/winner/LSTM/{}.csv'.format(season))\n", "    dataset = pd.read_csv(path)\n", "    dataset['DATE'] = pd.to_datetime(dataset['DATE'])\n", "    X = dataset.iloc[:, 1:-1].values\n", "    y = dataset.iloc[:, -1].values\n", "    \n", "    return X,y\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def lstm_no_validation(season = '2018-2018'):\n", "    X, y = import_dataset(season)\n", "    \n", "    features, labels, info = parse_lstm_data(X, y)\n", "    \n", "    lstm, history = build_lstm(features, labels)\n", "    \n", "    return lstm, history\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def lstm(season = '2018-2018'):\n", "    X, y = import_dataset(season)\n", "    \n", "    \" Splitting the dataset into the Training set and Test set \"\n", "    \n", "    features, labels, info = parse_lstm_data(X, y)\n", "    \n", "    from sklearn.model_selection import train_test_split\n", "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25)\n", "    X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.2)\n", "    \n", "    lstm, history = build_lstm(X_train, y_train, X_test, y_test)\n", "    \n", "    \" Overfit check \"\n", "    \n", "    fig, axs = plt.subplots(2)\n", "    \n", "    # create accuracy sublpot\n", "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n", "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n", "    axs[0].set_ylabel(\"Accuracy\")\n", "    axs[0].legend(loc=\"lower right\")\n", "    axs[0].set_title(\"Accuracy eval (LSTM)\")\n", "    \n", "    # create error sublpot\n", "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n", "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n", "    axs[1].set_ylabel(\"Error\")\n", "    axs[1].set_xlabel(\"Epoch\")\n", "    axs[1].legend(loc=\"upper right\")\n", "    axs[1].set_title(\"Error eval (LSTM)\")\n", "    \n", "    plt.show()\n", "    \n", "    \" Predicting single result \"\n", "    \n", "    # print(lstm.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)\n", "    \n", "    \" Predicting results with a margin of certainty\"\n", "    \n", "    y_pred = lstm.predict(X_validation)\n", "    \n", "    rows = y_pred.shape[0]\n", "    cols = y_pred.shape[1]\n", "    \n", "    y_less_risk_test = []\n", "    y_less_risk_pred = []\n", "    \n", "    for y in range(0, rows -1):\n", "      if y_pred[y][0] <= 0.4 or y_pred[y][0] >= 0.6:\n", "        y_less_risk_test.append(y_validation[y])\n", "        y_less_risk_pred.append(y_pred[y] > 0.5)\n", "    \n", "    y_less_risk_test = np.array(y_less_risk_test)\n", "    y_less_risk_pred = np.array(y_less_risk_pred)\n", "    \n", "    from sklearn.metrics import confusion_matrix, accuracy_score\n", "    cm = confusion_matrix(y_less_risk_test, y_less_risk_pred)\n", "    # print('Predictions with a margin of certainty for the validation set')\n", "    # print(cm)\n", "    # print(accuracy_score(y_less_risk_test, y_less_risk_pred))\n", "    \n", "    y_pred = (y_pred > 0.5)\n", "    \n", "    \" Predicting results for all data\"\n", "    \n", "    from sklearn.metrics import confusion_matrix, accuracy_score\n", "    cm = confusion_matrix(y_validation, y_pred)\n", "    # print('\\nPredictions for the entire validation set')\n", "    acc_score = accuracy_score(y_validation, y_pred)\n", "    # print(cm)\n", "    # print(acc_score)\n", "    \n", "    return cm, acc_score, lstm\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    lstm()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}